---
layout: post
title: "Traveling Sales Person"
date: 2018-12-12
excerpt: '
    We discuss the Traveling Sales Person problem and its complexity. There being no polynomial solution to the original problem in time different techniques have been applied to by using the practicality of domain problems. We explore algorithms that are Approximate but with a guaranteed bound in error and Randomized ones that find a local minimum. In the later parts we experiment on the complexity of input to understand the physical running time and error distributions.
'
comments: true
mathjax: true
tags: algorithms complexity
feature: "{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/Salesman.png"
---
<!-- <h2 id="introduction">Introduction</h2> -->
<p>The Traveling Salesman Problem (TSP), a NP-hard graph problem, is known as one of the most widely studied combinatorial optimization problems. One common interpretation of TSP is that of determining the shortest tour of a salesman through n cities. Some practical applications of TSP include: a) computer writing(Lenstra and Rinnooy Kan<span class="citation">(Lenstra and Kan 1975)</span>, 1975)), b) wall paper cutting(Garfinkel, 1977)<span class="citation">(Garfinkel 1977)</span>, c)hole punching (Reinelt, 1989)<span class="citation">(Reinelt 1992)</span>, d) Job sequencing, e) Dartboard design (Eiselt and Laporte, 1991)<span class="citation">(Blazewicz et al. 1991)</span>, f) Crystallography (Bland and Shallcross, 1989)<span class="citation">(Bland and Shallcross 1989)</span>.</p>
<p>There are two types of TSP : symmetric and asymmetric TSP. In the symmetric TSP, the distance between two vertices is the same in either direction. In the asymmetric TSP, paths might not exist in both directions or the distances in the opposite directions might be different. In this paper, we experimented and compared different algorithms to solve symmetric TSP, including Branch and Bound, Heuristics with Approximation Guarantees, Local Search and Hybrid Algorithm.</p>
<h2 id="problem">Problem</h2>
<p>The <span class="math inline"><em>T</em><em>S</em><em>P</em></span> problem is defined as follows: in a plane, Let <span class="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span> be a graph. <span class="math inline"><em>E</em></span> is a set of edges, and <span class="math inline"><em>V</em></span> is the set of n vertices. A cost function <span class="math inline"><em>c</em>(<em>u</em>, <em>v</em>)</span> is defined for every pair of vertices. Given these, Determine the shortest simple cycle that visits all n vertices once and only once.</p>
<h2 id="related-work">Related Work</h2>
<h3 id="branch-and-bound">Branch and Bound</h3>
<p>One of the most efficient exact solution for TSP is branch and bound. Branch and bound treated the set of the solutions as a rooted tree branching from the root. Before continue enumerating a branch on the tree (a subset of solutions), the branch and bound algorithm estimated the lower and upper bounds of the optimal solution, and discard a branch if this branch cannot produce a better solution. Because branch and bound algorithm can eliminate unnecessary cases), it usually has a better performance than exhaustive search.</p>
<p>Branch and Bound algorithm was proposed by Little et al.<span class="citation">(Little et al. 1963)</span> in conjunction with their TSP algorithm. After the first proposal of branch and bound algorithm by Little et al.<span class="citation">(Little et al. 1963)</span>, several improvements and refinements were later suggested by Helbig Hansen and Krarup (1974) <span class="citation">(Hansen and Krarup 1974)</span>, Smith and Thompson (1977) <span class="citation">(Smith, Srinivasan, and Thompson 1977)</span>, Volgenant and Jonker (1982) <span class="citation">(Volgenant and Jonker 1982)</span>, Gavish and Srikanth (1986) <span class="citation">(Gavish and Srikanth 1986)</span>, and Carpaneto, Fischettiand Toth (1989) <span class="citation">(Carpaneto et al. 1989)</span>. A couple of significant improvement of running time for TSP includes: 1) Balas and Christodes (1981) <span class="citation">(Balas and Christofides 1981)</span> provided optimal solutions to randomly generated 325-vertex TSPs in less than one minute on a CDC 7600. Carpaneto and Toth (1980) <span class="citation">(Carpaneto and Toth 1980)</span> reported they can consistently solve randomly generated 240-vertex problems in less than one minute on a CDC 6600.</p>
<h3 id="heuristics-with-approximation-guarantees">Heuristics with Approximation Guarantees</h3>
<p>MST-Approximation was firstly introduced in an innovative work <span class="citation">(Karp 1972)</span> on probabilistic analysis of algorithms, which showed that if in a <em>TPS</em>, the <em>N</em> nodes are selected independently and uniformly from unit square, then candidate tours whose cost is less than <em>1+E</em> of optimal (<span class="math inline"><em>E</em> &gt; 0</span>, arbitrarily small), can be found by the fixed dissection heuristic with high probability.</p>
<p>To calculate <em>MST-Approximation</em>, a <em>MST</em> graph for the <em>TSP</em> should be computed firstly. One lower bound for <em>MST</em> is the sum of edge in a <em>MST</em>. The <em>MST</em> graph can be converted to directed graph (edges doubled). If staring from a random point in the <em>MST</em> graph, when reaching the end of the tree, the visited node can be skipped by shortcut of back edges. The shortcuts would be less than original path, based on the triangle inequality. So the algorithm is guaranteed by approximation ratio of 2.</p>
<p>Christodes <span class="citation">(Christofides 1976)</span> optimized the <em>MST-approximation</em> algorithm which guaranteed that the cost of the computed a tour which is at most <span class="math inline">1.5</span> times the optimum. Furthermore, E. Sanjeev showed that <em>Euclidean TSP</em> has a <em>PTAS</em>. <span class="citation">(Arora 1996)</span>, which is a polynomial time algorithm that can approximate the problem within a factor <span class="math inline">1 + <em>E</em></span> (for each fixed E&gt;0).</p>
<h3 id="local-search">Local Search</h3>
<p>There are a number of different local search algorithms that have been developed to solve the TSP, such as <em>Hill climbing</em>, <em>random search</em>, <em>Simulated annealing</em>, <em>2-OPT</em> and <em>3-OPT</em>, <em>iterated local search</em>, and <em>tabu search</em>. It has been shown in several studies that local search algorithms can compute nearly optimal solutions in very quickly. For example, <em>LinKernighan</em> algorithm can achieve within 1% to 2% of the optimal solution, and <em>3-OPT</em> algorithm can get within 3% to 4% of the optimal solution <span class="citation">(D. S. Johnson and McGeoch 1997)</span>.</p>
<h2 id="algorithms">Algorithms</h2>
<h3 id="branch-and-bound-1">Branch and Bound</h3>
<p>A brute force way to find the minimal tour is to compute all possible tours and find the minimum weight one. Given that there are <span class="math inline"><em>N</em> − 1!</span> possible tours, it would take about <span class="math inline"><em>O</em>(<em>N</em> − 1!)</span> time to complete this. Consider a machine that makes <span class="math inline">10</span> billion computations every second (10 Ghz processor). It would take millions of years to compute even for 30 nodes.</p>
<div class="row">
<div class="col-md-8">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/Complexity.svg" alt="Physical Running Time" />
<p class="caption">Physical Running Time<span data-label="fig:physruntime"></span></p>
</div></div>
<p>Therefore it is not a feasible way to compute all possible tours. Hence we give this search among all possible solutions a structure and try to discard any tours as and when seen fit. For this we generate the tours organically by incrementally choosing (and rejecting) a node. Every time a node is chosen (or rejected) the problem becomes smaller, by accepting the existence (or absence) of the nodes that were chronologically considered before it. Hence we have to now explore a smaller problem.</p>
<h4 id="subproblem">Subproblem</h4>
<p>A subproblem for the TSP problem is created by considering only a subset of nodes. It is defined by the nodes already in tour.</p>
<h4 id="choose-and-expand">Choose and Expand</h4>
<p>As and when subproblems are created, after passing a evaluation criteria they enter a frontier configuration list. We then choose from this list the subproblem that has the maximum number of nodes in the tour.</p>
<p>In order to expand this subproblem and create new subproblems we consider adding each of the remaining nodes in the graph to the tour individually. If the tour already has <span class="math inline"><em>k</em></span> nodes then we are in a position to generate <span class="math inline"><em>N</em> − <em>k</em></span> new subproblems.</p>
<h4 id="pruning">Pruning</h4>
<p>Not all of these subproblems need to be expanded further. We evaluate if a subproblem is worth exploring. If the subproblem specifice tour has a cost that is greater than the best solution found so far, there is no need in exploring that branch as adding nodes to it would only increase the cost and it can in no way beat the best.</p>
<p>A second bound is placed on the lower bound possible for the subproblem. The smaller graph of the subproblem can generate a Hamiltonian path that has a cost at least greater than or equal to the two smallest edges from each node in the subgraph. This is the best that sub graph can contribute. If the total cost of the current tour of the branch with the lower bound is greater than the best cost so far, again it is not worth exploring any further as in no way it can beat the best cost.</p>
<p>The worst case complexity of this approach is not better than that of the Brute Force as in the worst case, no branch is pruned. But in practice we are able to generate solutions of <em><strong>good quality in reasonable amount of time</strong></em>. The worst case space taken up this algorithm is due to <span class="math inline"><em>F</em></span> storing all tours taking the complexity to <span class="math inline"><em>O</em>(<em>n</em>!)</span>. This also depends on the choice of the bounding function as they are the ones deciding how many nodes to be pruned.</p>
<p>Branch and Bound is bound to give us the optimal solution if run for eternity. While it is correct it is very slow.</p>
<h4 id="psuedocode">Psuedocode</h4>
<div class="row">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/BnB1.png" class="col-md-5">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/BnB2.png" class="col-md-6">
<!-- <img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/BnB3.png" class="col-md-4">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/BnB4.png" class="col-md-4"> -->
</div>
<h3 id="heuristics-with-approximation-guarantees-1">Heuristics with Approximation Guarantees</h3>
<p>Since we are dealing with a Euclidean problem, We employed the Christodes Algorithms <span class="citation">(Christofides 1976)</span> that approximates the optimal tour to be close to the MST of the graph. It gurantees a solution that is atmost <span class="math inline">1.5</span> times the optimal tour cost.</p>
<p>We know that Optimal tour can be at least as good as the MST. The idea is to build an MST and utilize as many edges in the MST as possible since MST is the lower bound on possible optimal tour.</p>
<p>Once the MST is built we pick the odd degree nodes in the MST. Compute the minimal match between these odd degree nodes in the original graph and add the minimal match edges to the MST generating the <span class="math inline"><em>T</em>*</span> graph.</p>
<p>We compute the Euler tour of <span class="math inline"><em>T</em>*</span> and then cut short on the already visited nodes. This generates a Hamiltonian Path on the original graph.</p>
<h4 id="psuedocode">Psuedocode</h4>
<div class="row">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/Approx1.png" class="col-md-6">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/Approx2.png" class="col-md-6">
<!-- <img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/Approx3.png" class="col-md-4"> -->
</div>
<h4 id="complexity">Complexity</h4>
<p>The generation of MST takes about <span class="math inline"><em>O</em>(<em>V</em><sup>2</sup><em>l</em><em>o</em><em>g</em>(<em>V</em>))</span> time. The minimal match algorithm runs in <span class="math inline"><em>O</em>(<em>E</em>)=<em>O</em>(<em>V</em><sup>2</sup>)</span> time and the loop at 9 takes a total time <span class="math inline"><em>O</em>(<em>V</em><sup>2</sup>)</span>. Hence this algorithm runs in <span class="math inline"><em>O</em>(<em>V</em><sup>2</sup><em>l</em><em>o</em><em>g</em>(<em>V</em>))</span> time and our experiments agree. It also utilized a space proportional to input <span class="math inline"><em>O</em>(<em>V</em>)</span>. In all of our runs the approximation is really good with the solution quality being <span class="math inline">&lt;1.2<em>O</em><em>P</em><em>T</em></span>.</p>
<h4 id="approximation-proof">Approximation Proof</h4>
<p>The cost of the approximate solution is at most <span class="math inline">1.5</span> times the optimal solution.</p>
<p>The solution(<span class="math inline"><em>S</em></span>) is obtained by cutshorting the Euler circuit over the graph(<span class="math inline"><em>T</em>*</span>) generated by combining the MST(<span class="math inline"><em>T</em></span>) and the edges of minimal perfect matching(<span class="math inline"><em>M</em></span>) of the odd degree vertices(<span class="math inline"><em>O</em></span>) of the MST. Therefore <br /><span class="math display"><em>c</em><em>o</em><em>s</em><em>t</em>(<em>S</em>)≤<em>c</em><em>o</em><em>s</em><em>t</em>(<em>T</em> * ) = <em>c</em><em>o</em><em>s</em><em>t</em>(<em>T</em>)+<em>c</em><em>o</em><em>s</em><em>t</em>(<em>M</em>)</span><br /></p>
<p>We already know that the optimal solution is at least as heavy as the MST. <span class="math inline"><em>c</em><em>o</em><em>s</em><em>t</em>(<em>T</em>)≤<em>O</em><em>P</em><em>T</em></span>. We are left to show <span class="math inline">$$cost(M)\le \frac{1}{2}OPT$$</span>.</p>
<p>Consider the minimum Hamiltonian path <span class="math inline"><em>H</em></span> of <span class="math inline"><em>M</em></span>. We can generate two perfect matchings from <span class="math inline"><em>H</em></span> by considering the nodes in <span class="math inline"><em>H</em></span> alternatively. At least one of them is at least half as heavy as <span class="math inline"><em>H</em></span>. Therefore the minimal matching of <span class="math inline"><em>M</em></span> is at most half as heavy as <span class="math inline"><em>H</em></span>. <span class="math inline">$$cost(M) \le \frac{1}{2} cost(H)$$</span>. <span class="math inline"><em>H</em></span> is bounded by the weight of the MST itself in the case when all the vertices are odd degree vertices. <span class="math inline"><em>c</em><em>o</em><em>s</em><em>t</em>(<em>H</em>)≤<em>c</em><em>o</em><em>s</em><em>t</em>(<em>T</em>)≤<em>O</em><em>P</em><em>T</em></span>. Hence we have <span class="math inline">$$cost(M) \le \frac{1}{2}OPT$$</span>.</p>
<p>Combining both of them we have <br /><span class="math display">$$cost(S) \le cost(T) + cost(M) = OPT + \frac{1}{2}OPT = \frac{3}{2}OPT$$</span><br /></p>
<h3 id="local-search-1-2-opt">Local Search 1 (2-OPT)</h3>
<p><em>2-OPT</em> exchange was first proposed by Croes in 1958 <span class="citation">(Croes 1958)</span>. It can optimize both symmetric and asymmetric instances. For our project we will discuss only symmetric instances. There is no guaranty that <em>2-OPT</em> will result in global optimum but will provide us local optimum of given initial solution for Euclidean problems. In Figure [fig:cross_resolution], If <span class="math inline">{<em>A</em>, <em>B</em>, <em>F</em>, <em>H</em>, <em>G</em>, <em>E</em>, <em>D</em>, <em>C</em>, <em>A</em>}</span> is initial solution, cost of solution can reduced by replacing edges <span class="math inline">(<em>B</em>, <em>F</em>)</span> and <span class="math inline">(<em>E</em>, <em>D</em>)</span> with <span class="math inline">(<em>B</em>, <em>E</em>)</span> and <span class="math inline">(<em>F</em>, <em>D</em>)</span>. That will give us a new solution <span class="math inline">{<em>A</em>, <em>B</em>, <em>E</em>, <em>G</em>, <em>H</em>, <em>F</em>, <em>D</em>, <em>C</em>, <em>A</em>}</span>. Notice the edges that we replaced are diagonals of a rectangle and larger than sides. That is we replace diagonals with sides and hence reducing the total cost. However, new diagonals could be introduced.</p>

<div class="row">
<div class="col-md-3"></div>

<div class="col-md-6">
<img style="width:66%" src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/cross1.png"  /> 
<img style="width:66%" src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/cross2.png" />
<p class="caption">Cross resolution in a euclidean problem<span data-label="fig:cross_resolution"></span></p>

</div>
<div class="col-md-3"></div>
</div>
<ul>
<li><p>Find a trial Solution <span class="math inline"><em>s</em> ∈ <em>S</em></span>, for which <span class="math inline"><em>M</em>(<em>s</em>)</span> is as small as we can make it at a first try.</p></li>
<li><p>Apply some transformations, called <em>inversions</em>, which transforms this trial solutions into some other elements of <span class="math inline"><em>S</em></span>, whose measures are progressive smaller.</p></li>
<li><p>Check <span class="math inline"><em>C</em></span> for elements which might be included in the final <span class="math inline"><em>s</em></span> at an advantage. If there are any such elements try to find a transformation which decreases the measure of the sequence.</p></li>
</ul>
<p>Repeat above procedure till there is no improvement.</p>
<p><em>Restart:</em> Since 2-OPT can only find local optimum within neighborhood of given input solution, It will be a good idea to run 2-OPT with different initial solutions. This is called restart. In our current implementation, we chose initial solutions randomly.</p>
<h4 id="limitations">Limitations</h4>
<p>This algorithm sequentially searches through all neighbors and explores the best neighbor’s neighborhood. That makes this algorithm slow and it won’t reach global optimum if the number of nodes is more than 150 in a reasonable time.</p>
<h4 id="strengths">Strengths</h4>
<p>This algorithm is first of it’s kind and easy to understand. It is a great help for students to understand the local search paradigm.</p>
<h4 id="psuedocode">Psuedocode</h4>
<div class="row">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LSA.png" class="col-md-6">
<!-- </div><div class="row">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS12.png" class="col-md-6">
</div><div class="row">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS13.png" class="col-md-6"> -->
</div>
<h4 id="time-and-space-complexity">Time and Space complexity</h4>
<p>A two OPT swap is upper bounded by O(V). We are doing <span class="math inline"><em>V</em><sup>2</sup></span> 2-opt swaps for any random input. That is the cost of the searching single neighborhood is <span class="math inline"><em>O</em>(<em>V</em><sup>3</sup>)</span>. The algorithm doesn’t terminate after searching the single neighborhood. If it found a better solution within the neighbors of our input, algorithm updates input to found better solution and iterates till there is no improvement or time out. It is hard to tell practical upper bound of the improvement loop. Since we have time out, in our case that is a constant time. But it can end up taking exponential time to find the local minimum. And hence our specific 2-opt implementation with timeout has a complexity of <span class="math inline"><em>O</em>(<em>V</em><sup>3</sup>)</span> per search. Note that without the timeout, complexity will be much higher.</p>
<h3 id="local-search-2-simulated-annealing">Local Search 2 (Simulated Annealing)</h3>
<p><em>S</em>imulated annealing (SA), a widely used technique for solving combinatorial optimization problems, mimics the chaotic movement of atoms at different temperatures while exploring the search space. In SA, solutions with worse quality are accepted with a probability in order to escape from local optima. The probability to accept worse solution keeps decreasing during the process of searching the solution space in order to in order to stabilize and end at a high-quality solution.</p>
<p><em>T</em>o apply SA to solve TSP, we explored the TSP’s search space using 3-opt exchange, i.e. three unique and non-neighboring edges are deleted, and their nodes are partially swapped. To increase the probability of escaping from local maximal, we used reheating when the temperature reaches a threshold at low temperatures. The pseudo code and manually-tuned parameters of our SA algorithm for TSP is listed below.</p>
<h4 id="psuedocode">Psuedocode</h4>
<div class="row">
<!-- <div class="col-md-4"></div> -->
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS21.png" class="col-md-6">
<div class="col-md-4"></div>
</div>
<h4 id="complexities">Complexities</h4>
<p>The space complexity of the SA algorithm of O(V*V) because we used adjacency list to store the graph, and we just use constant space in the algorithm. For the time complexity, we have three while loops at step 10, 11 and 13 respectively. Within each of the while loop it is only constant time, so the time complexity it O(Number of reheats * number of temperature drops * number of iterations in each temperature).</p>
<p>The strength of SA is that it allows escaping from the local optimal at the high temperature, and find the better solutions at the low temperature. Therefore SA can quickly converge to a good solution. The weakness of SA is that it requires hyper-parameters tuning for different problems in order to get good solution. In the experiments, we manually tuned(Eye balled) the hyper-parameters for SA, and used the hyper-parameters which resulted in the best solution in this project.</p>
<h3 id="hybrid-algorithm">Hybrid Algorithm</h3>
<p>We combined the Approximation algorithm with the 2-OPT Local search algorithm. Since Local Search is capable of finding a good solution given a good neighborhood, we hypothesized that by Locally searching around the tours generated by the Approximation algorithm we might get even better solutions.</p>
<p>However we found that the best solution may not be in the neighborhood of the best approximation. So we let the approximation algorithm generate all possible solutions and use them as the starting points for Local Search. As was noted earlier, the approximation algorithm can generate <span class="math inline"><em>V</em></span> approximate solutions each of whose cost is <span class="math inline">≤1.5<em>O</em><em>P</em><em>T</em></span>. We then locally search among these <span class="math inline"><em>V</em></span> solutio ns in a random order for the best possible tour. We cannot place an upper bound on the neighborhood search time of the local search algorithm.</p>
<h2 id="experiments">Experiments</h2>
<p>We run the algorithms on a single machine and note the physical running times and the quality of solution generated. Table [table_results] shows these values for various instances of the problem. The algorithms are cut off at 600s, while noting the best solution as and when found.</p>
<!-- <p>Details of the server (titan.cc.gatech.edu)</p>
<ul>
<li><p>CPU: Intel(R) Xeon(R) CPU E5-4627 v3 @ 2.60GHz</p></li>
<li><p>RAM: 1000 GiB</p></li>
<li><p>Language: Python2.7</p></li>
</ul> -->
<h3 id="branch-and-bound-2">Branch and Bound</h3>
<p>The performance of Branch and bound within limits of our of experiment is really bad. The problems start with very high error rates and hardly seem to decrease as time progresses for problems with a large number of nodes due to its exponential nature.</p>
<div class="row">
<div class="col-md-8">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/BnB_D_Comparison.svg" alt="Performance of Branch &amp; Bound algorithm" />
<p class="caption">Performance of Branch &amp; Bound algorithm<span data-label="fig:BnB_Comparision"></span></p>
</div></div>

<div class="row">
<div class="col-md-8">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/BnB_Error.svg" alt="Final Errors for Branch and Bound" />
<p class="caption">Final Errors for Branch and Bound<span data-label="fig:BnB_ErrorBar"></span></p>
</div></div>

<h3 id="heuristics-with-approximation-guarantees-2">Heuristics with Approximation Guarantees</h3>
<div class="row">
<div class="col-md-8">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/Approx_TimeComplexity.svg" alt="Time Complexity of Approximation Algorithm" />
<p class="caption">Time Complexity of Approximation Algorithm<span data-label="fig:Approx_Time"></span></p>
</div></div>

<p>The time complexity of this algorithm is computed be to be <span class="math inline"><em>O</em>(<em>V</em><sup>2</sup><em>l</em><em>o</em><em>g</em>(<em>V</em>))</span>. As can be seen the theoretical complexity aligns well with the physical run time. Here the <span class="math inline"><em>O</em>(<em>V</em><sup>2</sup><em>l</em><em>o</em><em>g</em>(<em>V</em>))</span> line is drawn by considering <span class="math inline">1.5 * 10<sup>−2</sup><em>V</em><sup>2</sup><em>l</em><em>o</em><em>g</em>(<em>V</em>)</span>.</p>
<div class="row">
<div class="col-md-8">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/Approx_Error.svg" alt="Final Errors for Approximation" />
<p class="caption">Final Errors for Approximation<span data-label="fig:Approx_ErrorBar"></span></p>
</div></div>

<p>Interestingly the Approximation algorithms runs with an error rate of atmost <span class="math inline">13%⟹</span> quality <span class="math inline">≤1.13<em>O</em><em>P</em><em>T</em></span>. However it didn’t generate optimal solutions even for the smaller size problems.</p>
<h3 id="local-search-1-2-opt-1">Local Search 1 (2-OPT)</h3>
<p>2-OPT performs a lot better than the Branch and Bound empirically. It quickly converges to a low error rate and computes the optimal tour.</p>
<div class="row">
<div class="col-md-8">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS1_D_Comparison.svg" alt="Performance of 2-OPT algorithm" />
<p class="caption">Performance of 2-OPT algorithm<span data-label="fig:2OPT_Comparision"></span></p>
</div></div>
<p>Figure: [fig:2OPT_Comparision] shows the behavior with respect to time for different sizes of inputs. As the dimensions increase the final error rates increase, but remain manageable. In the 10 minute running time limits that we set, except for <em>Roanoke</em> other instances reach a solution within <span class="math inline">5.9%−0%</span> of the optimal solution.</p>
<p>In order to better understand how quickly 2-OPT gets closer to the optimal solution, we measure the time when solution is within <span class="math inline">10%</span> of the optimal solution for different input dimensions. We also plot the QRTD and QSD for Toronto.</p>
<div class="row">
<div class="col-md-8">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS1_10pct.svg" alt="Time Taken to bring error below 10% by 2OPT" />
<p class="caption">Time Taken to bring error below 10% by 2OPT<span data-label="fig:2OPT_10pct_comparision"></span></p>
</div></div>
<p>The above plot suggests that the time taken to achieve a maximum error(here <span class="math inline">10%</span>) is an exponential in input dimensions, which is understandable as we may have to Locally Search more random tours.</p>
<div class="row">
<div class="col-md-8">
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS1_QRTD.svg" alt="2-OPT QRTD on Toronto (N=109)" />
<p class="caption">2-OPT QRTD on Toronto (N=109)<span data-label="fig:LS1_QRTD"></span></p>
</div></div>

<div class="row">

    <div class="col-md-8" >
        <img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS1_SQD.svg" alt="2-OPT SQD on Toronto (N=109)" />
        <p class="caption">2-OPT SQD on Toronto (N=109)<span data-label="fig:LS1_SQD"></span></p>
    </div>

</div>

<p>The QRTD plot is a good way visualize and understand the problem limits. More specifically for error rates, if we have requirement of say atmost <span class="math inline">5%</span> error then from above plot we can say, errors less than <span class="math inline">5%</span> are achieved about <span class="math inline">60%</span> of the runs within <span class="math inline">300</span>s.</p>
<p>Similarly SQD can be used to understand the error rates when there is time limit to solve. If our timelimit is <span class="math inline">80</span>s, we can look at the <span class="math inline">80</span>s and decided whether to use this algorithm or not based on the probability of achieving a certain error.</p>
<div class="row">

    <div class="col-md-8" >
       <img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS1_error_time_boxplot_Toronto.svg" alt="Variation of Time to obtain certain errors for 2OPT while running on Toronto N=109" />
        <p class="caption">Variation of Time to obtain certain errors for 2OPT while running on Toronto <span class="math inline"><em>N</em> = 109</span><span data-label="fig:LS1_Boxplot"></span></p>
    </div>

</div>

<p>This box plot gives us a good idea of the variance in time due to the randomness in the algorithm. The x axis is of errors. It is in essence showing us that to achieve an error rate between <span class="math inline">0 − 2%</span> (represented by error rate <span class="math inline">1%</span> on the x axis) the algorithm takes about <span class="math inline">487</span>s (Median), while it may vary between <span class="math inline">550</span>s to <span class="math inline">100</span>s.</p>
<h3 id="local-search-2-simulated-annealing-1">Local Search 2 (Simulated Annealing)</h3>
<p>SA also performed much better than branch and bound. Compared to 2-OPT, SA converged slower when input data dimension is small, but faster when input data dimension is large. SA can reach a final solution within <span class="math inline">5.5%</span> to <span class="math inline">0%</span> of the optimal solution.</p>
<p>Figure 6 shows the performance of SA in 10 minutes running time.</p>

<div class="row">

    <div class="col-md-8" >
    <img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS2_D_Comparison.svg" alt="Performance of Simulated Annealing" />
<p class="caption">Performance of Simulated Annealing<span data-label="fig:SA_Comparision"></span></p>
    </div>

</div>

<p>To understand how quickly SA gets closer to the optimal solution, we also measured the time when SA found a solution within <span class="math inline">10%</span> of the optimal solution for different dimensions.</p>

<div class="row">

    <div class="col-md-8" >
    <img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS2_10pct.svg" alt="Time Taken to bring error below 10% by SA" />

<p class="caption">Time Taken to bring error below 10% by SA<span data-label="fig:SA_10pct_comparision"></span></p>
    </div>

</div>




<div class="row">

    <div class="col-md-8" >
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS2_QRTD.svg" alt="Simulated Annealing QRTD on Toronto (N=109)" />
<p class="caption">Simulated Annealing QRTD on Toronto (N=109)<span data-label="fig:SA_QRTD"></span></p>
    </div>

</div>


<div class="row">

    <div class="col-md-8" >
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS2_SQD.svg" alt="Simulated Annealing SQD on Toronto (N=109)" />
<p class="caption">Simulated Annealing SQD on Toronto (N=109)<span data-label="fig:SA_SQD"></span></p>
    </div>

</div>



<div class="row">

    <div class="col-md-8" >
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/LS2_error_time_boxplot_Toronto.svg" alt="Variation of Time to obtain certain errors for SA while running on Toronto N=109" />
<p class="caption">Variation of Time to obtain certain errors for SA while running on Toronto <span class="math inline"><em>N</em> = 109</span><span data-label="fig:LS2_Boxplot"></span></p>
    </div>

</div>

<p>The box plot (Figure [fig:LS2_Boxplot]) in essence showing us that to achieve an error rate between <span class="math inline">0 − 2%</span> (represented by error rate <span class="math inline">1%</span> on the x axis) the algorithm takes about <span class="math inline">238</span>s (Median). Comparing Figure [fig:LS2_Boxplot] and [fig:LS1_Boxplot] tells us that Simulated annealing can quickly reach lower error rates.</p>
<p>Considering Figures [fig:2OPT_10pct_comparision] and [fig:SA_10pct_comparision] we can say that SA approached the optimal solution faster than 2OPT.</p>
<p>It seems that SQD(Figure [fig:SA_SQD]) and QRTD(Figure [fig:SA_QRTD]) are both tending towards the upper right corner as compared to 2OPT search. Suggesting that the probabilities are higher for SA when considering similar limits, implying SA is a better algorithm with our parameters.</p>
<h3 id="hybrid-algorithm-1">Hybrid Algorithm</h3>

<div class="row">

    <div class="col-md-8" >
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/H1_Error.svg" alt="Error rates of the Hybrid algorithm" />
<p class="caption">Error rates of the Hybrid algorithm<span data-label="fig:H1_Comparision"></span></p>
    </div>

</div>


<div class="row">

    <div class="col-md-8" >
<img src="{{ site.url }}{{ site.baseurl }}/images/2018-12-12-tsp/H1_10pct.svg" alt="Time Taken to bring error below 10% by the Hybrid" />
<p class="caption">Time Taken to bring error below 10% by the Hybrid<span data-label="fig:H1_10pct"></span></p>
    </div>

</div>


<p>Comparing Figure [fig:H1_10pct], [fig:2OPT_10pct_comparision] and [fig:SA_10pct_comparision], it is clear that the hybrid algorithm reaches low error rates faster than SA or 2OPT. The hybrid algorithm achieve errors as low as <span class="math inline">5%</span> very quickly(within a second)</p>
<h2 id="discussion">Discussion</h2>
<p><em>Branch and Bound</em>, Optimal solution can be found in exponential time. Suitable for very small graphs. <em>Christofides approximation</em>, 1.5 approximation is guaranteed in polynomial time. If anyone looking for a real-time algorithm, that they may use from their user interface, this is the best choice given approximation is acceptable for the application. <em>2-OPT LS</em>, Only local optimum is guaranteed for a given input state. Notice that except while choosing an initial solution, there is no randomness in this algorithm. The algorithm examines each neighbor of the random initial start, moves to the best neighbor and repeats the process by choosing a new random start point. It is a stable sequential approach compared to random algorithms. <em>Simulated Annealing</em>, Neither local optimum nor global optimum is guaranteed. However, SA randomly samples from many neighborhoods through exploration(finds min out of those samples. Also, longer a neighborhood holds the so far min solution more it get explored). Because SA explores high number of neighborhoods than 2-opt, in practice, it’s more likely to find a better solution than 2-opt LS. In our experiments, SA outperformed 2-opt LS and find better solutions for most of our data sets. In addition, SA also converges faster to a solution than 2-opt in average based on our running time analysis. As shown in Figure 10 and Figure 15, the average time to obtain a certain solution at various error levels for SA is generally smaller than that for 2-opt. One possible reason is that 2-opt spends more time to find the local optimal, while SA is able to escape from the local optimal efficiently and find better solution faster. However, the variance of running time for SA is larger than 2-opt, probably due the randomness in SA. The high variance nature of SA makes it an inferior algorithm for TSP in practice than other algorithms.</p>
<p>As was discussed earlier our hybrid model is a combination of Approximate algorithm with the 2-OPT local search. We found that combining Approx with 2-OPT is better than combining Approx with SA. Since many good solutions are within or in nearby neighborhoods of approximation solution, 2-OPT stable sequential approach will explore them. On other hand due to randomness randomness in SA, it spends a lot of time exploring not so good far away neighborhoods.</p>
<p>Note that standalone SA is better than stand alone 2-OPT. Approximation + 2 opt is better than Approximation + SA. Approximation + 2-OPT is our final approach that we choose for competition.</p>
<p>Finally, hypothesized that Branch and bound can be optimized with an upper bound which is the solution of our above hybrid model. However, Branch and bound was still too slow and we didn’t pursue it any further.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We are convinced that there are ways to cope up with NP-completeness. Use BnB for small sets. Use approximation for a real-time response (not for too big sets). Use LS on top of approximation for a better solution. Overall, this project is a great learning experience.</p>
<p>We have experimented on different approaches and weighed them against each other. This helped us realize a lot of interesting things that theoretical study may not have equipped us with. This is evident in the better performance of the Approximation algorithm being <span class="math inline">≤1.13<em>O</em><em>P</em><em>T</em></span> as opposed to the theoretical guarantee of <span class="math inline">≤1.5<em>O</em><em>P</em><em>T</em></span>. It was also a surprise when we realized coupling two good algorithms doesn’t automatically give us the best. We are referring to the combination of Approx with 2-opt search as opposed to with Simulated Annealing although SA beats 2-opt.</p>
<h2 id="acknowledgements">Acknowledgements</h2>

<p>We have used an online solver <span class="citation">(Hans, n.d.)</span> based on the QSOPT linear programming solver <span class="citation">(Applegate et al. 2004)</span> to obtain the Optimal Solutions for each of our instances.</p>
<h2 id="references" class="unnumbered">References</h2>
<div id="refs" class="references">
<div id="ref-applegate2004qsopt">
<p>Applegate, D, W Cook, S Dash, and M Mevenkamp. 2004. “QSopt Linear Programming Solver.” version.</p>
</div>
<div id="ref-arora1996polynomial">
<p>Arora, Sanjeev. 1996. “Polynomial Time Approximation Schemes for Euclidean Tsp and Other Geometric Problems.” In <em>Foundations of Computer Science, 1996. Proceedings., 37th Annual Symposium on</em>, 2–11. IEEE.</p>
</div>
<div id="ref-balas1981restricted">
<p>Balas, Egon, and Nicos Christofides. 1981. “A Restricted Lagrangean Approach to the Traveling Salesman Problem.” <em>Mathematical Programming</em> 21 (1). Springer: 19–46.</p>
</div>
<div id="ref-bland1989large">
<p>Bland, Robert G, and David F Shallcross. 1989. “Large Travelling Salesman Problems Arising from Experiments in X-Ray Crystallography: A Preliminary Report on Computation.” <em>Operations Research Letters</em> 8 (3). Elsevier: 125–28.</p>
</div>
<div id="ref-blazewicz1991scheduling">
<p>Blazewicz, Jacek, Horst A Eiselt, Gerd Finke, Gilbert Laporte, and Jan Weglarz. 1991. “Scheduling Tasks and Vehicles in a Flexible Manufacturing System.” <em>International Journal of Flexible Manufacturing Systems</em> 4 (1). Springer: 5–16.</p>
</div>
<div id="ref-carpaneto1980some">
<p>Carpaneto, Giorgio, and Paolo Toth. 1980. “Some New Branching and Bounding Criteria for the Asymmetric Travelling Salesman Problem.” <em>Management Science</em> 26 (7). INFORMS: 736–43.</p>
</div>
<div id="ref-carpaneto1989branch">
<p>Carpaneto, Giorgio, Mauro Dell’Amico, Matteo Fischetti, and Paolo Toth. 1989. “A Branch and Bound Algorithm for the Multiple Depot Vehicle Scheduling Problem.” <em>Networks</em> 19 (5). Wiley Online Library: 531–48.</p>
</div>
<div id="ref-christofides1976worst">
<p>Christofides, Nicos. 1976. “Worst-Case Analysis of a New Heuristic for the Travelling Salesman Problem.” Carnegie-Mellon Univ Pittsburgh Pa Management Sciences Research Group.</p>
</div>
<div id="ref-croes1958method">
<p>Croes, Georges A. 1958. “A Method for Solving Traveling-Salesman Problems.” <em>Operations Research</em> 6 (6). INFORMS: 791–812.</p>
</div>
<div id="ref-edmonds1973matching">
<p>Edmonds, Jack, and Ellis L Johnson. 1973. “Matching, Euler Tours and the Chinese Postman.” <em>Mathematical Programming</em> 5 (1). Springer: 88–124.</p>
</div>
<div id="ref-garfinkel1977minimizing">
<p>Garfinkel, Robert S. 1977. “Minimizing Wallpaper Waste, Part 1: A Class of Traveling Salesman Problems.” <em>Operations Research</em> 25 (5). INFORMS: 741–51.</p>
</div>
<div id="ref-gavish1986optimal">
<p>Gavish, Bezalel, and Kizhanathan Srikanth. 1986. “An Optimal Solution Method for Large-Scale Multiple Traveling Salesmen Problems.” <em>Operations Research</em> 34 (5). INFORMS: 698–717.</p>
</div>
<div id="ref-hans">
<p>Hans. n.d. “NEOS Server for Concorde.” <em>Neos_server</em>. <a href="https://neos-server.org/neos/solvers/co:concorde/TSP.html" class="uri">https://neos-server.org/neos/solvers/co:concorde/TSP.html</a>.</p>
</div>
<div id="ref-hansen1974improvements">
<p>Hansen, Keld Helbig, and Jakob Krarup. 1974. “Improvements of the Held—Karp Algorithm for the Symmetric Traveling-Salesman Problem.” <em>Mathematical Programming</em> 7 (1). Springer: 87–96.</p>
</div>
<div id="ref-johnson1997traveling">
<p>Johnson, David S, and Lyle A McGeoch. 1997. “The Traveling Salesman Problem: A Case Study in Local Optimization.” <em>Local Search in Combinatorial Optimization</em> 1. Chichester, UK: 215–310.</p>
</div>
<div id="ref-karp1972reducibility">
<p>Karp, Richard M. 1972. “Reducibility Among Combinatorial Problems.” In <em>Complexity of Computer Computations</em>, 85–103. Springer.</p>
</div>
<div id="ref-lenstra1975some">
<p>Lenstra, Jan Karel, and AHG Rinnooy Kan. 1975. “Some Simple Applications of the Travelling Salesman Problem.” <em>Journal of the Operational Research Society</em> 26 (4). Taylor &amp; Francis: 717–33.</p>
</div>
<div id="ref-little1963algorithm">
<p>Little, John DC, Katta G Murty, Dura W Sweeney, and Caroline Karel. 1963. “An Algorithm for the Traveling Salesman Problem.” <em>Operations Research</em> 11 (6). INFORMS: 972–89.</p>
</div>
<div id="ref-reinelt1992fast">
<p>Reinelt, Gerhard. 1992. “Fast Heuristics for Large Geometric Traveling Salesman Problems.” <em>ORSA Journal on Computing</em> 4 (2). INFORMS: 206–17.</p>
</div>
<div id="ref-smith1977computational">
<p>Smith, Theunis HC, V Srinivasan, and GL Thompson. 1977. “Computational Performance of Three Subtour Elimination Algorithms for Solving Asymmetric Traveling Salesman Problems.” In <em>Annals of Discrete Mathematics</em>, 1:495–506. Elsevier.</p>
</div>
<div id="ref-volgenant1982branch">
<p>Volgenant, Ton, and Roy Jonker. 1982. “A Branch and Bound Algorithm for the Symmetric Traveling Salesman Problem Based on the 1-Tree Relaxation.” <em>European Journal of Operational Research</em> 9 (1). Elsevier: 83–89.</p>
</div>
</div>